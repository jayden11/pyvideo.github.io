<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>http://pyvideo.org/</link><description></description><lastBuildDate>Sat, 08 Oct 2016 00:00:00 +0000</lastBuildDate><item><title>Scaling up to Big Data Devops for Data Science</title><link>http://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marck Vaisman</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</guid><category>big data</category><category>Data</category><category>data science</category><category>devops</category><category>scaling</category><category>science</category></item><item><title>Tratando datos más allá de los límites de la memoria</title><link>http://pyvideo.org/pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la era del 'Big Data' se necesitan cantidades cada vez más grandes de memoria (RAM) para tratar y analizar estos datos. Pero tarde o temprano se llega a unos límites por encima de los cuales no se puede (o es muy caro) pasar.&lt;/p&gt;
&lt;p&gt;El compresor Blosc (blosc.org) y el contenedor de datos bcolz (bcolz.blosc.org), usan las capacidades de los ordenadores modernos (caches, procesadores multihilo y SSDs) para permitir tratar datos más allá los límites de la memoria.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesc Alted</dc:creator><pubDate>Tue, 02 Feb 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-02-02:pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html</guid><category>workshop</category><category>big data</category><category>blosc</category><category>bcolz</category></item><item><title>Usando contenedores para Big Data</title><link>http://pyvideo.org/pycon-es-2015/usando-contenedores-para-big-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la actualidad existe una variedad bastante grande de contenedores de datos para almacenar grandes cantidades de datos en Python, tanto en memoria como en disco. En mi taller pasaremos revista a unos cuantos de los más útiles, empezando por los más básicos y generales (listas, diccionarios, NumPy/ndarray, pandas/DataFrames) a los más especializados (RDBMS, PyTables/Table/HDF5, bcolz/carray/ctable). Durante el camino se darán pistas de cuando usar unos u otros dependiendo del caso de uso.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Francesc Alted</dc:creator><pubDate>Tue, 02 Feb 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-02-02:pycon-es-2015/usando-contenedores-para-big-data.html</guid><category>workshop</category><category>big data</category><category>numpy</category><category>pandas</category><category>pytables</category><category>bcolz</category></item><item><title>MapReduce mit Disco</title><link>http://pyvideo.org/pycon-de-2013/mapreduce-mit-disco.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Mit dem MapReduce-Verfahren können massive Datenmengen auf einem
Rechencluster verarbeitet werden. Namensgeber und wichtige Bestandteile
sind eine Map- und eine Reduce-Phase. Diese werden jeweils
parallelisiert ausgeführt und ermöglichen somit eine optimale Auslastung
der vorhandenen Ressourcen. Im Vergleich zu einer entsprechenden
sequentiellen Implementierung können dadurch große Zeiteinsparungen
erreicht werden.&lt;/p&gt;
&lt;p&gt;Mit dem freien Disco-Framework können MapReduce-Aufgaben leicht in
Python erstellt werden. Beim Zugriff auf die Eingabedaten werden
verschiedene Protokolle unterstützt. Während der Ausführung kann der
Zustand des Rechenclusters sowie der Fortschritt der einzelnen Aufgaben
mit Hilfe einer Weboberfläche überwacht werden. Ein verteiltes
Dateisystem, das Disco Distributed Filesystem (DDFS), wird zur
Speicherung der Zwischen- und Endergebnisse verwendet.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Dr. Jan Morlock</dc:creator><pubDate>Thu, 17 Oct 2013 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2013-10-17:pycon-de-2013/mapreduce-mit-disco.html</guid><category>big data</category><category>disco</category><category>mapreduce</category><category>parallelisierung</category></item></channel></rss>
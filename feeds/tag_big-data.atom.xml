<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/tag_big-data.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2016-10-08T00:00:00+00:00</updated><entry><title>Scaling up to Big Data Devops for Data Science</title><link href="http://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html" rel="alternate"></link><published>2016-10-08T00:00:00+00:00</published><updated>2016-10-08T00:00:00+00:00</updated><author><name>Marck Vaisman</name></author><id>tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</summary><category term="big data"></category><category term="Data"></category><category term="data science"></category><category term="devops"></category><category term="scaling"></category><category term="science"></category></entry><entry><title>Tratando datos más allá de los límites de la memoria</title><link href="http://pyvideo.org/pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html" rel="alternate"></link><published>2016-02-02T00:00:00+00:00</published><updated>2016-02-02T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-02-02:pycon-es-2015/tratando-datos-mas-alla-de-los-limites-de-la-memoria.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la era del 'Big Data' se necesitan cantidades cada vez más grandes de memoria (RAM) para tratar y analizar estos datos. Pero tarde o temprano se llega a unos límites por encima de los cuales no se puede (o es muy caro) pasar.&lt;/p&gt;
&lt;p&gt;El compresor Blosc (blosc.org) y el contenedor de datos bcolz (bcolz.blosc.org), usan las capacidades de los ordenadores modernos (caches, procesadores multihilo y SSDs) para permitir tratar datos más allá los límites de la memoria.&lt;/p&gt;
</summary><category term="workshop"></category><category term="big data"></category><category term="blosc"></category><category term="bcolz"></category></entry><entry><title>Usando contenedores para Big Data</title><link href="http://pyvideo.org/pycon-es-2015/usando-contenedores-para-big-data.html" rel="alternate"></link><published>2016-02-02T00:00:00+00:00</published><updated>2016-02-02T00:00:00+00:00</updated><author><name>Francesc Alted</name></author><id>tag:pyvideo.org,2016-02-02:pycon-es-2015/usando-contenedores-para-big-data.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;En la actualidad existe una variedad bastante grande de contenedores de datos para almacenar grandes cantidades de datos en Python, tanto en memoria como en disco. En mi taller pasaremos revista a unos cuantos de los más útiles, empezando por los más básicos y generales (listas, diccionarios, NumPy/ndarray, pandas/DataFrames) a los más especializados (RDBMS, PyTables/Table/HDF5, bcolz/carray/ctable). Durante el camino se darán pistas de cuando usar unos u otros dependiendo del caso de uso.&lt;/p&gt;
</summary><category term="workshop"></category><category term="big data"></category><category term="numpy"></category><category term="pandas"></category><category term="pytables"></category><category term="bcolz"></category></entry><entry><title>MapReduce mit Disco</title><link href="http://pyvideo.org/pycon-de-2013/mapreduce-mit-disco.html" rel="alternate"></link><published>2013-10-17T00:00:00+00:00</published><updated>2013-10-17T00:00:00+00:00</updated><author><name>Dr. Jan Morlock</name></author><id>tag:pyvideo.org,2013-10-17:pycon-de-2013/mapreduce-mit-disco.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Mit dem MapReduce-Verfahren können massive Datenmengen auf einem
Rechencluster verarbeitet werden. Namensgeber und wichtige Bestandteile
sind eine Map- und eine Reduce-Phase. Diese werden jeweils
parallelisiert ausgeführt und ermöglichen somit eine optimale Auslastung
der vorhandenen Ressourcen. Im Vergleich zu einer entsprechenden
sequentiellen Implementierung können dadurch große Zeiteinsparungen
erreicht werden.&lt;/p&gt;
&lt;p&gt;Mit dem freien Disco-Framework können MapReduce-Aufgaben leicht in
Python erstellt werden. Beim Zugriff auf die Eingabedaten werden
verschiedene Protokolle unterstützt. Während der Ausführung kann der
Zustand des Rechenclusters sowie der Fortschritt der einzelnen Aufgaben
mit Hilfe einer Weboberfläche überwacht werden. Ein verteiltes
Dateisystem, das Disco Distributed Filesystem (DDFS), wird zur
Speicherung der Zwischen- und Endergebnisse verwendet.&lt;/p&gt;
</summary><category term="big data"></category><category term="disco"></category><category term="mapreduce"></category><category term="parallelisierung"></category></entry></feed>
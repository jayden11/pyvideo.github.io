<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>http://pyvideo.org/</link><description></description><lastBuildDate>Sun, 09 Oct 2016 00:00:00 +0000</lastBuildDate><item><title>Improving PySpark Performance Spark performance beyond the JVM</title><link>http://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Holden Karau</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</guid><category>jvm</category><category>performance</category><category>pyspark</category><category>spark</category></item><item><title>Hassle Free ETL with PySpark</title><link>http://pyvideo.org/pygotham-2016/hassle-free-etl-with-pyspark.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;While the models of data science get all the press, the real work is in the maze of data preprocessing and pipelines. The goal of this talk is to get a glimpse into how you can use Python and the distributed power of Spark to simplify your (data) life, ditch the ETL boilerplate and get to the insights. Weâ€™ll intro PySpark and considerations in ETL jobs with respect to code structure and performance.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Howley</dc:creator><pubDate>Sat, 16 Jul 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-07-16:pygotham-2016/hassle-free-etl-with-pyspark.html</guid><category>spark</category></item></channel></rss>
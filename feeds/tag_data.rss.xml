<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0"><channel><title>PyVideo.org</title><link>http://pyvideo.org/</link><description></description><lastBuildDate>Sun, 09 Oct 2016 00:00:00 +0000</lastBuildDate><item><title>Becoming a Data Scientist Advice From My Podcast Guests</title><link>http://pyvideo.org/pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Overwhelmed by the vast resources (of varying quality) available online for learning data science? In this talk, I compile resources from data scientists on twitter, advice from guests of my podcast, and some of my own experience to help get you started on the path to Becoming a Data Scientist.&lt;/p&gt;
&lt;p&gt;The options for learning data science online are vast and overwhelming, but it is possible to find great resources that work well for you and learn data science without going back to school if you know how to approach it.&lt;/p&gt;
&lt;p&gt;On my &amp;quot;Becoming a Data Scientist&amp;quot; podcast, I have interviewed 17 data scientists (or those on the way to becoming data scientists) about their career paths and how they learned data science. I also interact with hundreds of data scientists regularly on Twitter. In this talk, I compile the frequent advice and the best resources, and give my answers to some common questions about how to become a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Renee Teate</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/becoming-a-data-scientist-advice-from-my-podcast-guests.html</guid><category>Data</category></item><item><title>Data Transformation: A Framework for Exploratory Data Analysis</title><link>http://pyvideo.org/pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Exploratory data analysis plays a critical role in the job of every data scientist, but very few have a structured process or framework for exploring data quickly and efficiently. This talk will introduce the exploratory framework I use in my day-to-day work and will walk attendees through a practical example of how to use the framework to unlock hidden insights with the help of Python libraries.&lt;/p&gt;
&lt;p&gt;At the heart of data analysis, there lies a need to understand the real world entities being represented in the data. Every data set we encounter is an attempt to capture a slice of our complex world and communicate some information about it in a way that has potential to be informative to humans, machines, or both. Moving from basic analyses to advanced analytics requires the ability to imagine multiple ways of conceptualizing the composition of entities and the relationships present in our data. It also requires the realization that different levels of aggregation, disaggregation, and transformation can open up new pathways to understanding our data and identifying the valuable insights it contains.&lt;/p&gt;
&lt;p&gt;In this talk, we’ll discuss several ways to think about the composition and representation of our data. We’ll also demonstrate a series of methods that leverage tools like networks, hierarchical aggregations, and unsupervised clustering to visually explore our data, transform it to discover new insights, help frame analytical problems and questions, and even improve machine learning model performance. In exploring these approaches, and with the help of Python libraries such as Pandas, Scikit-Learn, Seaborn, and NetworkX, we will provide a practical framework for thinking creatively and visually about your data and unlocking latent value and insights hidden deep beneath its surface.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tony Ojeda</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/data-transformation-a-framework-for-exploratory-data-analysis.html</guid><category>analysis</category><category>Data</category><category>Data Analysis</category><category>framework</category></item><item><title>Dev Ops meets Data Science Taking models from prototype to production with Docker</title><link>http://pyvideo.org/pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;We present the evolution of a model to a production API that can scale to large e-commerce needs. On the journey we discuss metrics of success and how to use the Kubernetes cluster manager and associated tools for deploy. In addition to the use of these tools we highlight how to make use of the cluster management system for further testing and experimentation with your models.&lt;/p&gt;
&lt;p&gt;The chasm between data science and dev ops is often wide and impenetrable, but the two fields have more in common than meets the eye. Every data scientist will be able to lean in and help their career by investing in a basic understanding the basic principles of dev ops. In this talk I present the notions of service level indicators, objectives, and agreements. I cover the rigorous monitoring and testing of services. Finally we demonstrate how to build a basic data science workflow and push to production level APIs with Docker and Kubernetes.&lt;/p&gt;
&lt;p&gt;Kubernetes is an opinionated container cluster manager with an easy to use, robust interface. It can be use on very small and very large clusters. Docker is a container system that allows one to build code in an isolated environment. Paired with a container manager such as Kubernetes we are able to manage millions of instances as needed for a production deployment. These tools are two of many different options but are considered among the best open source solutions available.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Andy Terrel</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/dev-ops-meets-data-science-taking-models-from-prototype-to-production-with-docker.html</guid><category>Data</category><category>data science</category><category>docker</category><category>models</category><category>science</category></item><item><title>Keynote: Extending from Open to Usable: A Commerce Data Conundrum</title><link>http://pyvideo.org/pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Keynote: Extending from Open to Usable: A Commerce Data Conundrum&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Star Ying</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-extending-from-open-to-usable-a-commerce-data-conundrum.html</guid><category>Data</category></item><item><title>Keynote: The Culture of Data Transformation</title><link>http://pyvideo.org/pydata-dc-2016/keynote-the-culture-of-data-transformation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">SriSatish Ambati</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/keynote-the-culture-of-data-transformation.html</guid><category>Culture</category><category>Data</category></item><item><title>Open Data Dashboards &amp; Python Web Scraping</title><link>http://pyvideo.org/pydata-dc-2016/open-data-dashboards-python-web-scraping.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Distilling a world of data down to a few key indicators can be an effective way of keeping an audience informed, and this concept is at the heart of a good dashboard. This talk will cover a few methods of scraping and reshaping open data for dashboard visualization, to automate the boring stuff so you have more time and energy to focus on the analysis and content.&lt;/p&gt;
&lt;p&gt;This talk will cover a basic scenario of curating open data into visualizations for an audience. The main goal is to automate data scraping/downloading and reshaping. I use python to automate data gathering, and Tableau and D3 as visualization tools -- but the process can be applied to numerous analytical/visualization suites.&lt;/p&gt;
&lt;p&gt;I'll discuss situations where a dashboard makes sense (and when one doesn't). I will make a case also that automation makes for a more seamless data gathering and updating process, but not always for smarter data analysis.&lt;/p&gt;
&lt;p&gt;Some python packages I'll cover for web scraping and downloading/reshaping open data include: openpyxl, pandas, xlsxwriter, and BeautifulSoup. I'll also touch on APIs.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marie Whittaker</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/open-data-dashboards-python-web-scraping.html</guid><category>Data</category><category>scraping</category><category>web</category></item><item><title>Triaging Feedback Form Data</title><link>http://pyvideo.org/pydata-dc-2016/triaging-feedback-form-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk will cover how to use predictive modeling on unstructured text data including feedback form, social media or chat message data to triage issues in order to prevent future problems with a service, platform or user interface using NLP techniques in Python and R.&lt;/p&gt;
&lt;p&gt;Companies gain useful insights about their users from feedback form and other unstructured text data including live chat messages. Even though they are read and responded to, often such data is ignored when thinking about larger scale trend analysis and this can result in missed insight about how users react to a product or service. Sometimes analysis is being done by looking at changes in user sentiment or other heuristics, however it could be taken a step further by applying predictive modeling in attempt to recognize areas that need more attention and support. While you can use predictive modeling on network and log data, that is looking at how the hardware is handling your users requests, not how it's being perceived by users. By predicting areas where users are having difficulty whether it's with the UI or with the platform's response time you can triage these areas of concern to prevent future cases of negative perception. This talk will cover how to utilize common NLP tools used to gather and process the features in Python then will use R to perform trend analysis and predictive modeling then use the results to triage what areas should be focused on in the future.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Stephanie Kim</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/triaging-feedback-form-data.html</guid><category>Data</category></item><item><title>You got your engineering in my Data Science: Addressing the reproducibility crisis</title><link>http://pyvideo.org/pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering"&gt;http://www.slideshare.net/jonbodner/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis-with-software-engineering&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. Unfortunately, multiple recent studies have been found to be unreliable and non-reproducible. Adopting techniques from software engineering might help mitigate some of these problems.&lt;/p&gt;
&lt;p&gt;Data science is the backbone of modern scientific discovery and industry. It makes sense of everything from cancer trials to package delivery logistics. But all is not well with data science. Over the past decade, multiple studies have been found to be unreliable and non-reproducible when other scientists tried to recreate their results. This is due to a variety of factors, including fraud, pressure to publish, improper data handling practices, and bugs in analytic tools.&lt;/p&gt;
&lt;p&gt;The problems faced by data science mirror problems that software engineering has been trying to solve. While there are no silver bullets to guarantee quality software, techniques have been developed over time that have improved quality and reliability. Some of these techniques, including open source, version control, automation, and fuzzing could be adapted to the data science domain to improve reliability and help address the reproducibility crisis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jon Bodner</dc:creator><pubDate>Sun, 09 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-09:pydata-dc-2016/you-got-your-engineering-in-my-data-science-addressing-the-reproducibility-crisis.html</guid><category>Data</category><category>data science</category><category>engineering</category><category>reproducibility</category></item><item><title>Creating Python Data Pipelines in the Cloud</title><link>http://pyvideo.org/pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;My talk will be an analysis of the various approaches to creating data pipelines the public cloud using Python.I will compare and contrast using various Python libraries such as Luigi, Airflow and native cloud frameworks such as Cloud Dataflow (Google), AWS Data Pipeline to create a real world data pipeline in Amazon AWS and Google Compute Engine.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Femi Anthony</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/creating-python-data-pipelines-in-the-cloud.html</guid><category>Cloud</category><category>Data</category></item><item><title>Data Sciencing While Female</title><link>http://pyvideo.org/pydata-dc-2016/data-sciencing-while-female.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;How can we increase the number of female data scientists in the workplace? By building a community. Dr. Amanda Traud was the only woman on her data science team when she started the group Women Data Scientists DC. In one year, the group grew to over 1,000 members. Dr. Traud will discuss the ups and downs of being a woman in data science and how to encourage and include more women in the field.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mandi Traud</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/data-sciencing-while-female.html</guid><category>Data</category></item><item><title>Eat Your Vegetables Data Security for Data Scientists</title><link>http://pyvideo.org/pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists"&gt;http://www.slideshare.net/WilliamVoorhees1/eat-your-vegetables-data-security-for-data-scientists&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You've got data: lots of it. People want to get their hands on that data. You don't want that, so let's go over a few things you can do to dissuade attackers from getting their grubby mitts on your hard processed datastore. We'll cover the obvious things (spoiler alert: encryption) and then some advanced techniques for keeping data secure while still keeping it usable (that is to say, analyzable).&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Will Voorhees</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/eat-your-vegetables-data-security-for-data-scientists.html</guid><category>Data</category><category>Security</category></item><item><title>Forecasting critical food violations at restaurants using open data</title><link>http://pyvideo.org/pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Dc 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data"&gt;http://www.slideshare.net/NicoleDonnelly6/pydatadc-forecasting-critical-food-violations-at-restaurants-using-open-data&lt;/a&gt;
Github: &lt;a class="reference external" href="https://github.com/nd1/DC_RestaurantViolationForecasting"&gt;https://github.com/nd1/DC_RestaurantViolationForecasting&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;As many as 105 million Americans suffer from foodborne illness annually. In 2014, the City of Chicago began forecasting these outbreaks targeting limited health inspection resources toward likely sites, showing a 7 day improvement in locating critical violations at food establishments. This talk provides an end-to-end walkthrough of predicting critical violations in Washington, DC using Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Nicole Donnelly</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/forecasting-critical-food-violations-at-restaurants-using-open-data.html</guid><category>Data</category></item><item><title>How I learned to time travel, or, data pipelining and scheduling with Airflow</title><link>http://pyvideo.org/pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418"&gt;http://www.slideshare.net/PyData/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow-67650418&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data warehousing and analytics projects can, like ours, start out small - and fragile. With an organically growing mess of scripts glued together and triggered by cron jobs hiding on different servers, we needed better plumbing. After perusing the data pipelining landscape, we landed on Airflow, an Apache incubating batch processing pipelining and scheduler tool from Airbnb.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Laura Lorenz</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/how-i-learned-to-time-travel-or-data-pipelining-and-scheduling-with-airflow.html</guid><category>airflow</category><category>Data</category></item><item><title>Keynote: Become a Data Superhero How Data Can Change the World</title><link>http://pyvideo.org/pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;The capacity to gather and interpret data can be low for many nonprofits. Working together, data scientists and organizations can be a world-changing combination. Byte Back has found a way to use data analysis for good and will help you learn how to tap into your own superpowers.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Elizabeth Lindsey</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-become-a-data-superhero-how-data-can-change-the-world.html</guid><category>Data</category></item><item><title>Keynote: Building a Data Driven Dialogue From Filling Potholes to Disrupting the Cycle</title><link>http://pyvideo.org/pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Kelly Jin</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-building-a-data-driven-dialogue-from-filling-potholes-to-disrupting-the-cycle.html</guid><category>Data</category></item><item><title>Keynote: How Open Data Science Opens the World of Innovation</title><link>http://pyvideo.org/pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Innovation today appears to be instantaneous in large part due to open source technology. Open Data Science is no exception. Python, a pillar in the Open Data Science bedrock, is well positioned to harvest innovation in software and with Anaconda, it’s also well positioned to capitalize on the latest hardware innovations. Anaconda and Intel are blazing a path for the Python community to take advantage of cognitive computing, including machine learning and deep learning.&lt;/p&gt;
&lt;p&gt;In this keynote, Peter and Robert will talk about how Open Data Science––a connected ecosystem of data, analytics and compute––streamlines the path to high performance and innovation to achieve breakthrough results.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Robert Cohn</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/keynote-how-open-data-science-opens-the-world-of-innovation.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>Promoting a data driven culture in a world of microservices</title><link>http://pyvideo.org/pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment"&gt;http://www.slideshare.net/PyData/promoting-a-data-driven-culture-in-a-microservices-environment&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;At Hudl, we give every employee full access to our data warehouse, and over 50% of our employees have personally written a query against it. In this talk, I discuss our journey to democratize our data. I touch on technical and non-technical challenges, including the tools we use and the structure of our teams.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alex DeBrie</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/promoting-a-data-driven-culture-in-a-world-of-microservices.html</guid><category>Culture</category><category>Data</category></item><item><title>Scaling up to Big Data Devops for Data Science</title><link>http://pyvideo.org/pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Scaling up R/Python from a single machine to a cluster environment can be tricky. While there are many tools available that make the launching of a cluster relatively easy, they are not focused or optimized to the specific use case of analytics but mostly on operations. Come and learn about devops tips and tricks to optimize your transition into the big data world as a data scientist.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Marck Vaisman</dc:creator><pubDate>Sat, 08 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-08:pydata-dc-2016/scaling-up-to-big-data-devops-for-data-science.html</guid><category>big data</category><category>Data</category><category>data science</category><category>devops</category><category>scaling</category><category>science</category></item><item><title>Building Your First Data Pipelines</title><link>http://pyvideo.org/pydata-dc-2016/building-your-first-data-pipelines.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;You need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks &amp;amp; Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.&lt;/p&gt;
&lt;p&gt;Data pipelines are hard. Too often we resort to retrofitting janky scripts, relying on keeping a readme up to data, etc.&lt;/p&gt;
&lt;p&gt;First, this proposal lays out the variety of tools that are available to build data pipelines. This talk will discuss why you should be using Luigi and how to use it in a variety of common use cases.&lt;/p&gt;
&lt;p&gt;Next, we will build a basic exploratory analysis using DC open data and Luigi to demonstrate the power of this concept and how it works with Jupyter.&lt;/p&gt;
&lt;p&gt;Finally, we'll retrofit a larger, more complex project to use Luigi to show how you can use it in bigger organizations.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hunter Owens</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/building-your-first-data-pipelines.html</guid><category>Data</category></item><item><title>Parallel Python Analyzing Large Data Sets</title><link>http://pyvideo.org/pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;Students will walk away with a high-level understanding of both parallel problems and how to reason about parallel computing frameworks. They will also walk away with hands-on experience using a variety of frameworks easily accessible from Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Aron Ahmadia</dc:creator><pubDate>Fri, 07 Oct 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-10-07:pydata-dc-2016/parallel-python-analyzing-large-data-sets.html</guid><category>Data</category><category>parallel</category><category>sets</category></item><item><title>Finding Driving Style Patterns in Caterpillar Machine Data</title><link>http://pyvideo.org/pydata-chicago-2016/finding-driving-style-patterns-in-caterpillar-machine-data.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://cat.app.box.com/s/1c4mvt8eayb5o7g2wp8nsdwbguhgersm"&gt;https://cat.app.box.com/s/1c4mvt8eayb5o7g2wp8nsdwbguhgersm&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Identifying predominant driving-style patterns in logged time series data of Caterpillar machines is daunting due to the nature and size of the data. However, insight gained from field data can deliver optimized powertrain control software and better machine performance. A solution for finding patterns was built using engineered features, dimensionality reduction, and unsupervised learning.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Benjamin Hodel</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/finding-driving-style-patterns-in-caterpillar-machine-data.html</guid><category>Data</category><category>patterns</category><category>style</category></item><item><title>Machine learning techniques for data cleaning</title><link>http://pyvideo.org/pydata-chicago-2016/machine-learning-techniques-for-data-cleaning.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1k42esoWoc_WezfPfQ5vxbHTsuFOvAshEusD-GFCElTQ/edit#slide=id.g166bf446d8_1_12"&gt;https://docs.google.com/presentation/d/1k42esoWoc_WezfPfQ5vxbHTsuFOvAshEusD-GFCElTQ/edit#slide=id.g166bf446d8_1_12&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Often, the most interesting datasets - data about people and organizations - are the messiest and most difficult to analyze. When data comes from multiple sources, or when data is entered manually, variation &amp;amp; ambiguity are inevitable. Learn about ways to infer structure and relationships in messy data, using open source Python libraries.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Cathy Deng</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/machine-learning-techniques-for-data-cleaning.html</guid><category>Data</category><category>learning</category><category>machine learning</category></item><item><title>Using Exploratory Data Analysis to Discover Patterns in Image and Document Collections</title><link>http://pyvideo.org/pydata-chicago-2016/using-exploratory-data-analysis-to-discover-patterns-in-image-and-document-collections.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://docs.google.com/presentation/d/1StRN0-_0x4BPkFFQ79GusgQAqXgwvJ1Fc8Tlu68YO4E/edit"&gt;https://docs.google.com/presentation/d/1StRN0-_0x4BPkFFQ79GusgQAqXgwvJ1Fc8Tlu68YO4E/edit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Exploratory Data Analysis (EDA) is one of the key sets of procedures for summarizing a dataset. In this talk we will develop an EDA procedure for large collections of documents and images (such as photo albums, emails, articles, etc). We will show features used from NLP and Deep Neural Nets and also introduce novel visualization techniques for large image collections using PyImagePlot.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Mehrdad Yazdani</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/using-exploratory-data-analysis-to-discover-patterns-in-image-and-document-collections.html</guid><category>analysis</category><category>Data</category><category>data analysis</category><category>patterns</category></item><item><title>When Worlds Collide: Productionalizing a Data Science Model</title><link>http://pyvideo.org/pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;On our first data science project at Shiftgig, the data science and engineering teams had to build software that was production-ready while maintaining the flexibility of a data science sandbox. Although these seem like irreconcilable goals, they forced us to improve inter-team communication and ultimately helped create a great product. We’ll walk through our process and the lessons we learned.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Tudor Radoaca</dc:creator><pubDate>Sun, 28 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-28:pydata-chicago-2016/when-worlds-collide-productionalizing-a-data-science-model.html</guid><category>Data</category><category>data science</category><category>model</category><category>science</category></item><item><title>Data Engineering Architecture at Simple</title><link>http://pyvideo.org/pydata-chicago-2016/data-engineering-architecture-at-simple.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;A walk through Simple's Data Engineering stack, including lessons learned and why we chose certain tools and languages for different parts of our infrastructure.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rob Story</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/data-engineering-architecture-at-simple.html</guid><category>architecture</category><category>Data</category><category>engineering</category></item><item><title>Keynote: Using Data Science for Social Good: Examples, Opportunities, and Challenges</title><link>http://pyvideo.org/pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Rayid Ghani</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/keynote-using-data-science-for-social-good-examples-opportunities-and-challenges.html</guid><category>Data</category><category>data science</category><category>science</category></item><item><title>What Data Analysts Wish Application Developers Knew</title><link>http://pyvideo.org/pydata-chicago-2016/what-data-analysts-wish-application-developers-knew.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Slides: &lt;a class="reference external" href="https://speakerdeck.com/alison985/what-data-analysts-wish-application-developers-knew"&gt;https://speakerdeck.com/alison985/what-data-analysts-wish-application-developers-knew&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Data analysts frequently do not get to participate in the app development process despite being some of its biggest stakeholders. This talk focuses on general guidelines and best practices for application developers on what they can do to optimize data content and quality available for analysis.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Alison Stanton</dc:creator><pubDate>Sat, 27 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-27:pydata-chicago-2016/what-data-analysts-wish-application-developers-knew.html</guid><category>Data</category></item><item><title>Luigi &amp; Data Pipelines</title><link>http://pyvideo.org/pydata-chicago-2016/luigi-data-pipelines.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Chicago 2016&lt;/p&gt;
&lt;p&gt;Github: &lt;a class="reference external" href="https://github.com/hunterowens/data-pipelines"&gt;https://github.com/hunterowens/data-pipelines&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;You need a data pipeline. This talk will discuss the lifecycle of projects using Jupyter notebooks &amp;amp; Luigi as a data pipeline management tool for a variety of projects, from greenfield to retrofitting complex systems. It will included a hands on demo.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Hunter Owens</dc:creator><pubDate>Fri, 26 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-26:pydata-chicago-2016/luigi-data-pipelines.html</guid><category>Data</category></item><item><title>Working with real-time data streams in Python</title><link>http://pyvideo.org/pycon-au-2016/working-with-real-time-data-streams-in-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;An increasing number of devices and applications are producing vast amounts of data in real time. This can include measurements, sensor readings, and performance data. Making this data useful often requires that we analyse and use the data in real time but this requires techniques to aggregate, filter, and smooth the data. Drawing on simple and well-tested techniques from mathematics and engineering allows us to solve these problems quickly and efficiently. This talk will describe how Python can be used to develop powerful capabilities for working with real-time data streams and provide simple examples you can start using yourself.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Lachlan Blackhall</dc:creator><pubDate>Fri, 12 Aug 2016 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2016-08-12:pycon-au-2016/working-with-real-time-data-streams-in-python.html</guid><category>Internet-of-Things</category><category>Data</category><category>Real-time</category><category>Kalman Filter</category></item><item><title>PyGotham 2011: Intro to Data Visualization</title><link>http://pyvideo.org/pygotham-2011/pygotham-2011--intro-to-data-visualization.html</link><description>&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;Have lots of data? Want to turn it into pictures to help you better
understand it or explain it to others? This session will address best
practices for encoding information through design, and will look at a
few ways of doing this in Python.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Julie Steele</dc:creator><pubDate>Mon, 01 Jan 1990 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,1990-01-01:pygotham-2011/pygotham-2011--intro-to-data-visualization.html</guid><category>data</category><category>pygotham</category><category>pygotham2011</category><category>visualization</category></item><item><title>Data mining and integration with Python</title><link>http://pyvideo.org/pytexas-2015/data-mining-and-integration-with-python.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;There is an abundance of data in social media sites (Wikipedia,
Facebook, Instagram, etc.) which can be accessed through web APIs. But
how do we know that the data from the Wikipedia article on &amp;quot;Golden Gate
Bridge&amp;quot; goes along with the data from &amp;quot;Golden Gate Bridge&amp;quot; Facebook
page? This represents an important question about integrating data from
various sources.&lt;/p&gt;
&lt;p&gt;In this talk, I'll outline important aspects of structured data mining,
integration and entity resolution methods in a scalable system.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Isaac Vidas</dc:creator><pubDate>Fri, 09 Oct 2015 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,2015-10-09:pytexas-2015/data-mining-and-integration-with-python.html</guid><category>Data</category><category>Data Analysis</category><category>data mining</category></item><item><title>Real-Time Django</title><link>http://pyvideo.org/djangocon-2011/djangocon-2011--real-time-django.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Real-Time Django&lt;/p&gt;
&lt;p&gt;Presented by Ben Slavin, Adam Miskiewicz&lt;/p&gt;
&lt;p&gt;The web is live. APIs give us access to continuously changing data. We
discuss ways to get real-time data into your app, how to handle data
processing and what to do when you get thousands of updates per second.&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Adam Miskiewicz</dc:creator><pubDate>Mon, 01 Jan 1990 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,1990-01-01:djangocon-2011/djangocon-2011--real-time-django.html</guid><category>data</category><category>djangocon</category><category>djangocon2011</category><category>realtime</category></item><item><title>Guy Kloss - Python Data Plotting and Visualisation Extravaganza</title><link>http://pyvideo.org/kiwi-pycon-2009/guy-kloss---python-data-plotting-and-visualisatio.html</link><description>&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Python Data Plotting and Visualization Extravaganza&lt;/p&gt;
&lt;p&gt;Presented by Guy Kloss&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;In various fields data is accumulated or produced. This can be
observation data, statistical data, simulation data, ... Information
like that can in many cases be much more easily analysed through the
user's eyes employing data visualisation. This talk is trying to dive
briefly into various means and tools to visually analyse data of
different qualities: time series, simple 2D plots, surface plots, volume
plots, quiver plots, etc.&lt;/p&gt;
&lt;p&gt;Outline&lt;/p&gt;
&lt;p&gt;I am planning on doing a &amp;quot;fly by&amp;quot; through the world of data
visualisation for different types of data using different tools. Types
of data:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;1D data and simple functions&lt;/li&gt;
&lt;li&gt;2D data for surface plots&lt;/li&gt;
&lt;li&gt;3D data through quiver plots, iso surfaces, and cutting planes&lt;/li&gt;
&lt;li&gt;n-D data through different means&lt;/li&gt;
&lt;li&gt;continuous and non-continuously structured data&lt;/li&gt;
&lt;li&gt;time series&lt;/li&gt;
&lt;li&gt;real time data visualisation/analysis&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tools that will probably appear in the demos and discussions:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;GNUplot&lt;/li&gt;
&lt;li&gt;matplotlib&lt;/li&gt;
&lt;li&gt;Mayavi2&lt;/li&gt;
&lt;li&gt;Visual Python&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[VIDEO HAS ISSUES: Sound and video are poor. Slides are hard to read.]&lt;/p&gt;
</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Guy Kloss</dc:creator><pubDate>Mon, 01 Jan 1990 00:00:00 +0000</pubDate><guid isPermaLink="false">tag:pyvideo.org,1990-01-01:kiwi-pycon-2009/guy-kloss---python-data-plotting-and-visualisatio.html</guid><category>data</category><category>gnuplot</category><category>kiwipycon</category><category>kiwipycon2009</category><category>matplotlib</category><category>mayavi2</category><category>plotting</category><category>visualpython</category></item></channel></rss>
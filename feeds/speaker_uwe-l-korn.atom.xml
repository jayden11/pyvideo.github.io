<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/speaker_uwe-l-korn.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2017-05-07T00:00:00+00:00</updated><entry><title>Efficient and portable DataFrame storage with Apache Parquet</title><link href="http://pyvideo.org/pydata-london-2017/efficient-and-portable-dataframe-storage-with-apache-parquet.html" rel="alternate"></link><published>2017-05-07T00:00:00+00:00</published><updated>2017-05-07T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2017-05-07:pydata-london-2017/efficient-and-portable-dataframe-storage-with-apache-parquet.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Filmed at PyData London 2017
www.pydata.org&lt;/p&gt;
&lt;p&gt;Description
Apache Parquet is the most used columnar data format in the big data processing space and recently gained Pandas support. It leverages various techniques to store data in a CPU and I/O efficient way and provides capabilities to push-down queries to the I/O layer. In this talk, it is shown how to use it in Python, detail its structure and present the portable usage with other tools.&lt;/p&gt;
&lt;p&gt;Abstract
Since its creation in 2013, Apache Parquet has risen to be the most widely used binary columnar storage format in the big data processing space. While supporting basic attributes of a columnar format like reading a subset of columns, it also leverages techniques to store the data efficiently while providing fast access. In addition the format is structured in such a fashion that when supplied to a query engine, Parquet provides indexing hints and statistics to quickly skip over chunks of irrelevant data.&lt;/p&gt;
&lt;p&gt;In recent months, efficient implementations to load and store Parquet files in Python became available, bringing the efficiency of the format to Pandas DataFrames. While this provides a new option to store DataFrames, it especially allows us to share data between Pandas and a lot of other popular systems like Apache Spark or Apache Impala. In this talk we will show the improvements that Parquet bring performance-wise but also will highlight important aspects of the format that make it portable and efficient for queries on large amount of data. As not all features are yet available in Python, an overview of the upcoming Python-specific improvements and how the Parquet format will be extended in general is given at the end of the talk&lt;/p&gt;
</summary></entry><entry><title>How Apache Arrow and Parquet boost cross-language interop</title><link href="http://pyvideo.org/pydata-paris-2016/how-apache-arrow-and-parquet-boost-cross-language-interop.html" rel="alternate"></link><published>2016-06-14T00:00:00+00:00</published><updated>2016-06-14T00:00:00+00:00</updated><author><name>Uwe L. Korn</name></author><id>tag:pyvideo.org,2016-06-14:pydata-paris-2016/how-apache-arrow-and-parquet-boost-cross-language-interop.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;How Apache Arrow and Parquet boost cross-language interop
by Uwe L. Korn (Blue Yonder)&lt;/p&gt;
</summary></entry></feed>
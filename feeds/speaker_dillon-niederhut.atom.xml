<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/speaker_dillon-niederhut.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2016-08-20T00:00:00+00:00</updated><entry><title>What to do when your data is large, but not big</title><link href="http://pyvideo.org/pybay-2016/what-to-do-when-your-data-is-large-but-not-big.html" rel="alternate"></link><published>2016-08-20T00:00:00+00:00</published><updated>2016-08-20T00:00:00+00:00</updated><author><name>Dillon Niederhut</name></author><id>tag:pyvideo.org,2016-08-20:pybay-2016/what-to-do-when-your-data-is-large-but-not-big.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;This talk will present strategies in Python for handling data that is too large to fit in memory and/or too slow to process in one thread, but small enough to still fit in one machine.
â€‹
Abstract
Unless you work at a large internet company, you probably don't have BIG data, but you might have LARGE data. Large data consume an unacceptable amount of time and memory when medium strategies are used, but also incur unnecessary financial and latency costs when big strategies are used. Two basic strategies for handling large data, chunking and parallelization, will be discussed with live coded examples in Python.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Bio:&lt;/strong&gt;
I'm a research scientist currently living in the Bay Area and working in neuroethology, human evolution, and natural language processing. I currently work at D-Lab, where I help researchers apply advances in computation to their research paradigms.&lt;/p&gt;
&lt;p&gt;&lt;a class="reference external" href="https://speakerdeck.com/pybay2016/dillon-niederhut-what-to-do-when-your-data-is-large-but-not-big"&gt;https://speakerdeck.com/pybay2016/dillon-niederhut-what-to-do-when-your-data-is-large-but-not-big&lt;/a&gt;&lt;/p&gt;
</summary></entry></feed>
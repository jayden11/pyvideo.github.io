<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/speaker_konark-modi.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2015-09-20T00:00:00+00:00</updated><entry><title>Getting schemas around semi-structured data using Avro</title><link href="http://pyvideo.org/pycon-uk-2015/getting-schemas-around-semi-structured-data-using-avro.html" rel="alternate"></link><published>2015-09-20T00:00:00+00:00</published><updated>2015-09-20T00:00:00+00:00</updated><author><name>Konark Modi</name></author><id>tag:pyvideo.org,2015-09-20:pycon-uk-2015/getting-schemas-around-semi-structured-data-using-avro.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;In this world of big-data where we are producing data of enormous
variety, velocity, volume and the nature being semi-structured we need
to put some rules around data being collected which are not rigid yet
allow efficient management of it right from collection layer.&lt;/p&gt;
&lt;p&gt;My talk is based on first-hand experience that me and my team had while
writing frameworks and data pipelines , we like anyone else started with
collecting data in JSON format. But soon started to run into problems
because of systems which were writing the data were abusing the
flexibility.&lt;/p&gt;
&lt;p&gt;More importantly how we can leverage Python with this and makes it
interoperable with various Big Data techologies.&lt;/p&gt;
&lt;p&gt;Schemas / Serialization is a layer not many people talk about , but in
my experience it is very useful and one can benefit from it in many many
ways.&lt;/p&gt;
&lt;p&gt;The agenda of the talk is as follows :&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;Need of a schema / serialization.&lt;/li&gt;
&lt;li&gt;Avro and why is it so awesome.&lt;/li&gt;
&lt;li&gt;Demo of creating some schemas and showcasing the features.&lt;/li&gt;
&lt;li&gt;How Avro serialized data can be used across different layers / tools
in the BigData pipeline. Be it Realtime processing system like Kafka
with Storm or Batch processing system like Hadoop, Hive etc.&lt;/li&gt;
&lt;li&gt;Setting up your AVRO schema repository to help people create,
distrubute and manage schemas. A pure Django based implementation.&lt;/li&gt;
&lt;li&gt;Do's and Don'ts .&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Who should Attend :&lt;/p&gt;
&lt;p&gt;I plan to keep it beginner friendly, any one who has been or is doing
data collection at any level can benefit from this talk.&lt;/p&gt;
</summary></entry><entry><title>Design considerations while Evaluating, Developing, Deploying a distributed task processing system</title><link href="http://pyvideo.org/europython-2014/design-considerations-while-evaluating-developin.html" rel="alternate"></link><published>2014-07-23T00:00:00+00:00</published><updated>2014-07-23T00:00:00+00:00</updated><author><name>Konark Modi</name></author><id>tag:pyvideo.org,2014-07-23:europython-2014/design-considerations-while-evaluating-developin.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;With the growing world of web, there are numerous use-cases which
require tasks to be executed in an asynchronous manner and in a
distributed fashion. Celery is one of the most robust, scalable,
extendable and easy-to-implement frameworks available for distributed
task processing. While developing applications using Celery, I have had
considerable experience in terms of what design choices one should be
aware of while evaluating an existing system or developing one's own
system from scratch.&lt;/p&gt;
</summary></entry><entry><title>Designing NRT(NearRealTime) stream processing systems: using python with Storm and Kafka</title><link href="http://pyvideo.org/europython-2014/designing-nrtnearrealtime-stream-processing-sys.html" rel="alternate"></link><published>2014-07-22T00:00:00+00:00</published><updated>2014-07-22T00:00:00+00:00</updated><author><name>Konark Modi</name></author><id>tag:pyvideo.org,2014-07-22:europython-2014/designing-nrtnearrealtime-stream-processing-sys.html</id><summary type="html">&lt;h3&gt;Summary&lt;/h3&gt;&lt;p&gt;The essence of near-real-time stream processing is to compute huge
volumes of data as it is received. This talk will focus on creating a
pipeline for collecting huge volumes of data using Kafka and processing
for near-real time computations using Storm.&lt;/p&gt;
</summary></entry></feed>
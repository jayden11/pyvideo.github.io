<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/tag_pyspark.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2016-10-09T00:00:00+00:00</updated><entry><title>Improving PySpark Performance Spark performance beyond the JVM</title><link href="http://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary><category term="jvm"></category><category term="performance"></category><category term="pyspark"></category><category term="spark"></category></entry><entry><title>PySpark in Practice</title><link href="http://pyvideo.org/pydata-berlin-2016/pyspark-in-practice.html" rel="alternate"></link><published>2016-06-01T00:00:00+00:00</published><updated>2016-06-01T00:00:00+00:00</updated><author><name>Ronert Obst</name></author><id>tag:pyvideo.org,2016-06-01:pydata-berlin-2016/pyspark-in-practice.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;In this talk we will share our best practices of using PySpark in numerous customer facing data science engagements. Topics covered in this talk are:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;Configuration&lt;/li&gt;
&lt;li&gt;Unit testing with PySpark&lt;/li&gt;
&lt;li&gt;Integration with SQL on Hadoop engines&lt;/li&gt;
&lt;li&gt;Data pipeline management and workflows&lt;/li&gt;
&lt;li&gt;Data Structures (RDDs vs. Data Frames vs. Data Sets)&lt;/li&gt;
&lt;li&gt;When to use MLlib vs scikit-learn&lt;/li&gt;
&lt;li&gt;Operationalisation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At Pivotal Labs we have many data science engagements on big data. Typical problems involve real-time data from sensors collected by telecom operators to GPS data produced by vehicle tracking systems. One widespread framework to solve those inherently difficult problems is Apache Spark. In this talk, we want to share our best practices with PySpark, Sparkâ€™s Python API, highlighting our experience as well as dos and dont's. In particular, we will focus on the whole data science pipeline from data ingestion, data munging and wrangling to the actual model building.&lt;/p&gt;
&lt;p&gt;Finally, many businesses have started to realise that there is no return on investment from data science if the models do not go into production. At Pivotal Labs, one our core principle is API first. Therefore, we will also talk how we put our models into production, sharing our hands-on knowledge in this field and also how this fits into test-driven development.&lt;/p&gt;
</summary><category term="pyspark"></category></entry><entry><title>Spotting trends and tailoring recommendations: PySpark on Big Data in fashion</title><link href="http://pyvideo.org/pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html" rel="alternate"></link><published>2016-06-01T00:00:00+00:00</published><updated>2016-06-01T00:00:00+00:00</updated><author><name>Martina Pugliese</name></author><id>tag:pyvideo.org,2016-06-01:pydata-berlin-2016/spotting-trends-and-tailoring-recommendations-pyspark-on-big-data-in-fashion.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Predicting what people like when they choose what to wear is a non-trivial task involving several ingredients. At Mallzee, the data is variegated, large and has to be processed quickly to produce recommendations on products, tailored to each user. We use PySpark to crunch large sets of different data and create models in order to generate robust and meaningful suggestions.&lt;/p&gt;
&lt;p&gt;Mallzee is a fashion app where people can see products (clothes, shoes and accessories) and decide whether they like them or not. They can also buy products and create feeds of preferred brands and categories of items. We have large amounts of data generated by the users when they scroll and search through products and we use it to understand the user. We want to give everyone meaningful recommendations on the items they might like, hence tailoring the experience to who they are. We build pseudo-intelligent algorithms capable of extracting the style profile of a user and we crunch products data to match items to the user based on a classification model. The model is validated and statistical analyses are performed to determine the tipping point when recommendations are valuable to the user. The talk will go through the steps we implement by showing how the full data stack of Python is used in achieving this goal and how it is interfaced with a Spark cluster through PySpark in order to run Machine Learning algorithms on big data.&lt;/p&gt;
</summary><category term="pyspark"></category></entry><entry><title>Machine Learning at Scale</title><link href="http://pyvideo.org/pydata-berlin-2016/machine-learning-at-scale.html" rel="alternate"></link><published>2016-05-31T00:00:00+00:00</published><updated>2016-05-31T00:00:00+00:00</updated><author><name>Nathan Epstein</name></author><id>tag:pyvideo.org,2016-05-31:pydata-berlin-2016/machine-learning-at-scale.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Berlin 2016&lt;/p&gt;
&lt;p&gt;Python machine learning libraries like scikit-learn are a fantastic resource but not always well suited to large datasets. How can we use Python for machine learning in such cases? This talk will introduce PySpark and MLlib as tools for distributed machine learning. We will discuss what these tools are, how they work, and cover some basic code examples of machine learning on a cluster.&lt;/p&gt;
&lt;ol class="arabic simple"&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Intro&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Why is scikit-learn not enough?&lt;/li&gt;
&lt;li&gt;What is Spark?&lt;/li&gt;
&lt;li&gt;What is MLlib?&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;Spark&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Overview of Spark&lt;/li&gt;
&lt;li&gt;Overview of PySpark&lt;/li&gt;
&lt;li&gt;PySpark code sample&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;li&gt;&lt;dl class="first docutils"&gt;
&lt;dt&gt;MLlib&lt;/dt&gt;
&lt;dd&gt;&lt;ol class="first last loweralpha"&gt;
&lt;li&gt;Overview of MLlib&lt;/li&gt;
&lt;li&gt;MLlib code samples&lt;/li&gt;
&lt;/ol&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;/li&gt;
&lt;/ol&gt;
</summary><category term="scikit-learn"></category><category term="pyspark"></category><category term="mllib"></category></entry><entry><title>PySpark - Data processing in Python on top of Apache Spark.</title><link href="http://pyvideo.org/europython-2015/pyspark-data-processing-in-python-on-top-of-apache-spark.html" rel="alternate"></link><published>2015-08-03T00:00:00+00:00</published><updated>2015-08-03T00:00:00+00:00</updated><author><name>Peter Hoffmann</name></author><id>tag:pyvideo.org,2015-08-03:europython-2015/pyspark-data-processing-in-python-on-top-of-apache-spark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Peter Hoffmann - PySpark - Data processing in Python on top of Apache Spark.
[EuroPython 2015]
[22 July 2015]
[Bilbao, Euskadi, Spain]&lt;/p&gt;
&lt;p&gt;[Apache Spark][1] is a computational engine for large-scale data processing. It
is responsible for scheduling, distribution and monitoring applications which
consist of many computational task across many worker machines on a computing
cluster.&lt;/p&gt;
&lt;p&gt;This Talk will give an overview of PySpark with a focus on Resilient
Distributed Datasets and the DataFrame API. While Spark Core itself is written
in Scala and runs on the JVM, PySpark exposes the Spark programming model to
Python. It defines an API for Resilient Distributed Datasets (RDDs). RDDs are a
distributed memory abstraction that lets programmers perform in-memory
computations on large clusters in a fault-tolerant manner. RDDs are immutable,
partitioned collections of objects. Transformations construct a new RDD from a
previous one. Actions compute a result based on an RDD. Multiple
computation steps
are expressed as directed acyclic graph (DAG). The DAG execution model is
a generalization of the Hadoop MapReduce computation model.&lt;/p&gt;
&lt;p&gt;The Spark DataFrame API was introduced in Spark 1.3. DataFrames envolve Spark's
RDD model and are inspired by Pandas and R data frames. The API provides
simplified operators for filtering, aggregating, and projecting over large
datasets. The DataFrame API supports diffferent data sources like JSON
datasources, Parquet files, Hive tables and JDBC database connections.&lt;/p&gt;
&lt;p&gt;Resources:&lt;/p&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;[An Architecture for Fast and General Data Processing on Large Clusters][2] Matei Zaharia&lt;/li&gt;
&lt;li&gt;[Spark][6] Cluster Computing with Working Sets - Matei Zaharia et al.&lt;/li&gt;
&lt;li&gt;[Resilient Distributed Datasets][5] A Fault-Tolerant Abstraction for In-Memory Cluster Computing -Matei Zaharia et al.&lt;/li&gt;
&lt;li&gt;[Learning Spark][3] Lightning Fast Big Data Analysis - Oreilly&lt;/li&gt;
&lt;li&gt;[Advanced Analytics with Spark][4] Patterns for Learning from Data at Scale - Oreilly&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;[1]: &lt;a class="reference external" href="https://spark.apache.org"&gt;https://spark.apache.org&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[2]: &lt;a class="reference external" href="http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf"&gt;http://www.eecs.berkeley.edu/Pubs/TechRpts/2014/EECS-2014-12.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[3]: &lt;a class="reference external" href="http://shop.oreilly.com/product/0636920028512.do"&gt;http://shop.oreilly.com/product/0636920028512.do&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: &lt;a class="reference external" href="http://shop.oreilly.com/product/0636920035091.do"&gt;http://shop.oreilly.com/product/0636920035091.do&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[5]: &lt;a class="reference external" href="https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf"&gt;https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[6]: &lt;a class="reference external" href="http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf"&gt;http://www.cs.berkeley.edu/~matei/papers/2010/hotcloud_spark.pdf&lt;/a&gt;&lt;/p&gt;
</summary><category term="pyspark"></category></entry></feed>
<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>PyVideo.org</title><link href="http://pyvideo.org/" rel="alternate"></link><link href="http://pyvideo.org/feeds/speaker_holden-karau.atom.xml" rel="self"></link><id>http://pyvideo.org/</id><updated>2017-04-09T00:00:00+00:00</updated><entry><title>Debugging PySpark - Pretending to make sense of JVM stack traces</title><link href="http://pyvideo.org/pydata-amsterdam-2017/debugging-pyspark-pretending-to-make-sense-of-jvm-stack-traces.html" rel="alternate"></link><published>2017-04-09T00:00:00+00:00</published><updated>2017-04-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2017-04-09:pydata-amsterdam-2017/debugging-pyspark-pretending-to-make-sense-of-jvm-stack-traces.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2017&lt;/p&gt;
&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common er&lt;/p&gt;
&lt;p&gt;Apache Spark is one of the most popular big data projects, offering greatly improved performance over traditional MapReduce models. Much of Apache Spark’s power comes from lazy evaluation along with intelligent pipelining, which can make debugging more challenging. This talk will examine how to debug Apache Spark applications, the different options for logging in PySpark, as well as some common errors and how to detect them.&lt;/p&gt;
&lt;p&gt;Spark’s own internal logging can often be quite verbose, and this talk will examine how to effectively search logs from Apache Spark to spot common problems. In addition to the internal logging, this talk will look at options for logging from within our program itself.&lt;/p&gt;
&lt;p&gt;Spark’s accumulators have gotten a bad rap because of how they interact in the event of cache misses or partial recomputes, but this talk will look at how to effectively use Spark’s current accumulators for debugging as well as a look to future for data property type accumulators which may be coming to Spark in future version.&lt;/p&gt;
&lt;p&gt;In addition to reading logs, and instrumenting our program with accumulators, Spark’s UI can be of great help for quickly detecting certain types of problems.&lt;/p&gt;
</summary></entry><entry><title>Sparkling Pandas- Letting Pandas Roam on Spark DataFrames</title><link href="http://pyvideo.org/pydata-seattle-2015/sparkling-pandas-letting-pandas-roam-on-spark-dataframes.html" rel="alternate"></link><published>2015-07-25T00:00:00+00:00</published><updated>2015-07-25T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2015-07-25:pydata-seattle-2015/sparkling-pandas-letting-pandas-roam-on-spark-dataframes.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark.&lt;/p&gt;
&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t naturally scale to more data than can fit in memory. PySpark is the Python API for Apache Spark that is designed to scale to huge amounts of data but lacks the natural expressiveness of Pandas. This talk introduces Sparkling Pandas, a library that brings together the best features of Pandas and PySpark; Expressiveness, speed, and scalability.&lt;/p&gt;
&lt;p&gt;While both Spark 1.3 and Pandas have classes named ‘DataFrame’ the Pandas DataFrame API is broader and not fully covered by the ‘DataFrame’ class in Spark. This talk will explore some of the differences between Spark’s DataFrames and Panda’s DataFrames and then examine some of the work done to implement Panda’s like DataFrames on top of Spark. In some cases, providing Pandas like functionality is computationally expensive in a distributed environment, and we will explore some techniques to minimize this cost.&lt;/p&gt;
&lt;p&gt;At the end of this talk you should have a better understanding of both Sparkling Pandas and Spark’s own DataFrames. Whether you end up using Sparkling Pandas or Spark directly, you will have a greater understanding of how to work with structured data in a distributed context using Apache Spark and familiar DataFrame APIs.&lt;/p&gt;
&lt;p&gt;Materials available here:
Slides: &lt;a class="reference external" href="http://www.slideshare.net/hkarau/sparkling-pandas-electric-bugaloo-py-data-seattle-2015"&gt;http://www.slideshare.net/hkarau/sparkling-pandas-electric-bugaloo-py-data-seattle-2015&lt;/a&gt;
Project github: &lt;a class="reference external" href="https://github.com/sparklingpandas/sparklingpandas"&gt;https://github.com/sparklingpandas/sparklingpandas&lt;/a&gt;
Project website: &lt;a class="reference external" href="http://sparklingpandas.com/"&gt;http://sparklingpandas.com/&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>A brief introduction to Distributed Computing with PySpark</title><link href="http://pyvideo.org/pydata-seattle-2015/a-brief-introduction-to-distributed-computing-with-pyspark.html" rel="alternate"></link><published>2015-07-24T00:00:00+00:00</published><updated>2015-07-24T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2015-07-24:pydata-seattle-2015/a-brief-introduction-to-distributed-computing-with-pyspark.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Apache Spark is a fast and general engine for distributed computing &amp;amp; big data processing with APIs in Scala, Java, Python, and R. This tutorial will briefly introduce PySpark (the Python API for Spark) with some hands-on-exercises combined with a quick introduction to Spark's core concepts. We will cover the obligatory wordcount example which comes in with every big-data tutorial, as well as discuss Spark's unique methods for handling node failure and other relevant internals. Then we will briefly look at how to access some of Spark's libraries (like Spark SQL &amp;amp; Spark ML) from Python. While Spark is available in a variety of languages this workshop will be focused on using Spark and Python together.&lt;/p&gt;
&lt;p&gt;This tutorial is intended for people new to Spark/PySpark, please install Spark (1.3.1 or later) from &lt;a class="reference external" href="http://spark.apache.org/downloads.html"&gt;http://spark.apache.org/downloads.html&lt;/a&gt; before class (we are working to have cluster resources available but having a local install is sufficient for the workshop and a good backup in case the WiFi isn't cooperating).&lt;/p&gt;
&lt;p&gt;Materials available here:
Slides: &lt;a class="reference external" href="http://www.slideshare.net/hkarau/a-really-really-fast-introduction-to-py-spark-lightning-fast-cluster-computing-with-python-1"&gt;http://www.slideshare.net/hkarau/a-really-really-fast-introduction-to-py-spark-lightning-fast-cluster-computing-with-python-1&lt;/a&gt;
Notebook:  &lt;a class="reference external" href="https://github.com/holdenk/intro-to-pyspark-demos/blob/master/ipython/Super-Fast-PySpark-Intro-PyData-Seattle-2015.ipynb"&gt;https://github.com/holdenk/intro-to-pyspark-demos/blob/master/ipython/Super-Fast-PySpark-Intro-PyData-Seattle-2015.ipynb&lt;/a&gt;&lt;/p&gt;
</summary></entry><entry><title>Improving PySpark Performance Spark performance beyond the JVM</title><link href="http://pyvideo.org/pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-10-09T00:00:00+00:00</published><updated>2016-10-09T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-10-09:pydata-dc-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData DC 2016&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames and traditional RDDs with Python. Looking at Spark 2.0; we examine how to mix functional transformations with relational queries for performance using the new (to PySpark) Dataset API. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary><category term="jvm"></category><category term="performance"></category><category term="pyspark"></category><category term="spark"></category></entry><entry><title>Improving PySpark Performance: Spark performance beyond the JVM</title><link href="http://pyvideo.org/pydata-amsterdam-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html" rel="alternate"></link><published>2016-03-26T00:00:00+00:00</published><updated>2016-03-26T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2016-03-26:pydata-amsterdam-2016/improving-pyspark-performance-spark-performance-beyond-the-jvm.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;PyData Amsterdam 2016&lt;/p&gt;
&lt;p&gt;Description&lt;/p&gt;
&lt;p&gt;This talk assumes you have a basic understanding of Spark (if not check out one of the intro videos on youtube - &lt;a class="reference external" href="http://bit.ly/hkPySpark"&gt;http://bit.ly/hkPySpark&lt;/a&gt; ) and takes us beyond the standard intro to explore what makes PySpark fast and how to best scale our PySpark jobs. If you are using Python and Spark together and want to get faster jobs - this is the talk for you.&lt;/p&gt;
&lt;p&gt;Abstract&lt;/p&gt;
&lt;p&gt;This talk covers a number of important topics for making scalable Apache Spark programs - from RDD re-use to considerations for working with Key/Value data, why avoiding groupByKey is important and more. We also include Python specific considerations, like the difference between DataFrames/Datasets and traditional RDDs with Python. We also explore some tricks to intermix Python and JVM code for cases where the performance overhead is too high.&lt;/p&gt;
</summary></entry><entry><title>Sparkling Pandas - using Apache Spark to scale Pandas</title><link href="http://pyvideo.org/pygotham-2014/sparkling-pandas-using-apache-spark-to-scale-pa.html" rel="alternate"></link><published>2014-09-17T00:00:00+00:00</published><updated>2014-09-17T00:00:00+00:00</updated><author><name>Holden Karau</name></author><id>tag:pyvideo.org,2014-09-17:pygotham-2014/sparkling-pandas-using-apache-spark-to-scale-pa.html</id><summary type="html">&lt;h3&gt;Description&lt;/h3&gt;&lt;p&gt;Pandas is a fast and expressive library for data analysis that doesn’t
naturally scale to more data than can fit in memory. PySpark is the
Python API for Apache Spark that is designed to scale to huge amounts of
data but lacks the natural expressiveness of Pandas. We will introduce
Sparkling Pandas, a new library that brings together the best features
of Pandas and PySpark; Expressiveness, speed, and scalability.&lt;/p&gt;
</summary></entry></feed>